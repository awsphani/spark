
https://resources.itversity.com/courses/cca-175-spark-and-hadoop-developer-certification-scala/lessons/cca-scala-fundamentals-scala/topic/cca-object-oriented-concepts-case-classes-scala/
udd cca-175-spark-and-hadoop-developer-certification-scala/learn/v4/content
SCALA INTRO:

Basic Programming Constructs

Declaring Variables
Invoking Functions
Conditional
While loop
For loop


Declaring Variables in Scala

Each and every variable in scala must be declared with either val or var
val is immutable and var is mutable

val i = 2 or var i=2

Invoking Functions

A function is a group of statements that perform a task.

prinlln is a function in scala
To invoke a function in scala we use its object name and function name

Example
val s = "Hello World"
s.toUpperCase

For Loop:epetitive structure which allows us to execute a block of code multiple times.

Example
for (i <- (1 to 100)) {
  println(i)
}

#Task – Sum of all the given numbers

var total = 0
for (element <- (1 to 100))
  total += element
  
#Task – Sum of even numbers from 1 to 100
var total = 0 
for (element <- (1 to 100))
  if(element % 2 ==0)
    total += element 
    
    
A while loop statement is executed repeatedly until the condition is false.

syntax

while(condition) { statement }

Display Sum of even numbers and Sum of odd numbers

var lb = 1
val ub = 100
var totalEven = 0
var totalOdd = 0
while(lb <= ub) {
  if(lb % 2 == 0)
    totalEven += lb
  else
    totalOdd += lb
  lb += 1
}


Functions and Anonymous Functions.

A function is a group of statements that perform a task
We need to give function name, arguments and argument types for regular functions
Functions are expressions (not statements)
Functions can be returned, passed as arguments.


Syntax

def functionName(parameters : typeofparameters) : returntypeoffunction = {  
// statements to be executed  
}

def sum(lb: Int, ub: Int)={
var total =0
for (element <- lb to ub) {
total += element
}
total
}

Problem Statement
Display the output for given statements

Sum of numbers in a given range
Sum of squares of numbers in a given range
Sum of cubes of numbers in a given range
Sum of multiples of 2 in a given range.

def sum(func: Int => Int, lb:Int, ub:Int)= {
var total = 0
for(element <- lb to ub)
{
total += func(element)
}
total
}

def id(i: Int)= i

def sqr(i: Int)= i * i

def cube(i: Int)= i * i * i

def double(i: Int)= i * 2

sum(id, 1, 10)

sum(sqr, 1, 10)

sum(cube, 1, 10)

sum(double, 1, 10)

sum(1,10)

Anonymous Functions

Anonymous functions need not have a name associated with it
Anonymous functions can be assigned to variables
Those variables can be passed as parameters
While invoking we need to provide the functionality for the parameters which are defined as functions (for parameter i in this case)

sum(i => i, 1, 10)
sum(i => i * i, 1, 10)
sum(i => i * i * i, 1, 10)
sum(i => i * 2, 1, 10)


Object Oriented Concepts – Classes

To define a class in Scala we declare keyword class in front of identifier. Class names should be capitalized.
syntax

class ClassName

Example

class Order(orderId: Int, orderDate: String, orderCustomerId: Int, orderStatus: String) {
println("I am inside Order Constructor")
}

Disassemble and Decompile Scala code
Use the javap command to disassemble a .class file to look at its signature.
Syntax

:javap -p className
//In this case its Order class

Object Creation
To create an object in scala we use new keyword
Syntax

val or var = new className(//parameters)

Creating a function inside a class
We can create a function inside a class
In this case we create an toString function which will start with override keyword. Since toString overrides the pre-defined toString method, it has to be tagged with the override flag.
By using toString function we can see the details of the object instead of byte code.

class Order(orderId:Int,orderDate:String,orderCustomerId:Int,orderStatus:String)
{
println("I am inside Order Constructor")
override def toString = "Order("+ orderId+ ","+ orderDate + ","+ orderCustomerId + ","+ orderStatus +")"
}

Object Creation for Order class
var order = new Order(1, "2013-10-01 00:00:00.00",100, "COMPLETE")

Passing variables in class instead of arguments
Example

class Order(val orderId:Int,val orderDate:String,val orderCustomerId:Int,val orderStatus:String) 
{
println("I am inside Order Constructor") override def toString = "Order("+ orderId+ ","+ orderDate + ","+ orderCustomerId + ","+ orderStatus +")" 
}

var order = new Order(1, "2013-10-01 00:00:00.00",100, "COMPLETE")

By default if we don’t set val or var in the parameters then those are called arguments and we cannot access their elements.


 special type of class called Object

Scala is more object-oriented than Java because in Scala, we cannot have static members. Instead, Scala has singleton objects
Object is a keyword which represents singleton class.

Creating a Scala object
Example

object HelloWorld {
  def main(args: Array[String]): Unit = {
    println("Hello World")
}
}
Invoking the above main method from REPL

HelloWorld.main(Array(" "))


Companion Objects

A companion object is an object with the same name as a class and is defined in the same source file as the associated file.

Example

class Order(val orderId:Int,val orderDate:String,val orderCustomerId:Int,val orderStatus:String)
{
 println("I am inside Order Constructor")
 override def toString = "Order("+ orderId+ ","+ orderDate + ","+ orderCustomerId + ","+ 
 orderStatus +")"
}
object Order {
 def apply(orderId: Int, orderDate: String, orderCustomerId: Int, orderStatus:String): Order = {
 new Order(orderId, orderDate, orderCustomerId, orderStatus)
}

val order = Order.apply(1, "2013-10-01 00:00:00.000", 100,"COMPLETE")



Case Class

Case classes can be pattern matched
Case classes automatically define hashcode and equals
Case classes automatically define getter methods for the constructor arguments(if we use var in the argument).

Example
case class Order(var orderId:Int,var orderDate:String,var orderCustomerId:Int,var orderStatus:String)
{
 println("I am inside Order Constructor")
 override def toString = "Order("+ orderId + "," + orderDate + "," + orderCustomerId + "," + orderStatus +")"
}

Object Creation
val order = Order(1, "2013-10-01 00:00:00.000", 100,"COMPLETE")




Scala collections are categorized into 3 types

Seq
Set
Map


Seq

Sequence have length
Elements in Sequence can be accessed using prefix
eg: scala.Array, scala.collection.immutable.List etc
Classes are divided into Seq classes and Buffer classes
Vector and List are considered as Seq classes and they are more frequently used
Array is special type of collection analogous to Java Array
We will get into the details of Array and List
Array is mutable while List is immutable
Seq have 2 sub traits – IndexedSeq and LinearSeq

Set
Set is iterable which contain unique elements (no duplicates)
As duplicates are not allowed, there is no length (Seq have both length and index)
Even though we can access data using index, it might not return same value always
By default Set is immutable, but there is mutable one as well
Let us see how we can use mutable and immutable Set in a demo
If the Set contain elements for which implicit Ordering defined, we can use SortedSet to sort the data

Map
Map is Iterable of Key and Value pairs
Each element can be defined as key -> value or (key, value)

Declaration of Array
val a = Array(1,2,3,4)

Declaration of List
val l = List(1,2,3,4)

Declaration of set
val s = Set(1,1,2,2,3,4)

Declaration of Map
val m = Map("Hello" -> 1 , "World" -> 2)



Basic Map Reduce Operations

Problem Statement
Get Sum of squares all Even Numbers in a given range

val l = (1 to 100).toList
val f = l.filter(ele => ele % 2 == 0)
val m = f.map(rec => rec * rec)
var total =0
for( e <- m) total + = e

use this command in the terminal for setting up data sets git clone https://github.com/dgadiraju/data.git


Read data from files
Convert into collection
Perform collection operation to preview the data
Run map-reduce operations

Reading data from files

To read files from a file system in scala there is a package called scala.io
In scala.io we have a package named Source which is used to read files from the file system
Important operations in Source package are as follows

Few APIs from Source to read data

fromFile is used to read data from a file
fromIterable is used to read data from collection
fromURI creates Source from a file with given file: URI
fromURL is used to read data from HTTP URL

import scala.io.Source

val orderItems = Source.fromFile("/data/retail_db/order_items/part-00000").getLines.toList
orderItems(0)
orderItems.take(10).foreach(println)
orderItems.size

val orderItemsFilter = orderItems.filter(orderItem => orderItem.split(",")(1).toInt == 2)
val orderItemsMap = orderItemsFilter.map(orderItem => orderItem.split(",")(4).toFloat)

orderItemsMap.sum

orderItemsMap.reduce((total, orderItemSubtotal) => total + orderItemSubtotal)

orderItemsMap.reduce(_ + _)


Tuples:
A tuple is another data structure in Scala
It can hold heterogeneous elements
Syntax for tuple val t = (1, "Hello World")
Another syntax for tuple val t: (Int, String) = (1, "Hello World")
Tuple has a handful of methods
Elements of tuples can be accessed using _ notation (t._1 will return 1 and t._2 return Hello World)
If there are 2 elements it is called also known as pair
Elements of a tuple can be collections and vice versa
Tuple with collection val t = (1, List(1, 2, 3, 4))
Collection with tuple  val t = List((1, "Hello"), (2, "World"))
We will use tuples quite extensively in Spark as a key value pair


Development Cycle – Developing Source Code
posted on APRIL 16, 2018

Topic Progress:                  
← Back to Lesson
As part of this topic, we will explore how to develop the application build a jar file and run it as a jar file.

Example
val orderItems = Source.fromFile("/data/retail_db/order_items/part-00000").getLines
val orderRevenue = orderItems.filter(oi => oi.split(",")(1).toInt == 2).
map(oi => oi.split(",")(4).toFloat).
reduce((t, v) => t + v)
Using vi editor to build the application
import scala.io.Source
object OrderRevenue {
def main(args: Array[String]) = {
 val orderItems = Source.fromFile("/home/dgadiraju/data/retail_db/order_items/part-00000").getLines
 val orderRevenue = orderItems.filter(oi => oi.split(",")(1).toInt == 2).
 filter(oi => oi.split(",")(1).toInt == 2).
 map(oi => oi.split(",")(4).toFloat).
 reduce((t, v) => t + v)
 println(orderRevenue)
 }
}
To run the program use below command

scala src/main/scala/orderRevenue.scala


Note
We should never run the program like above because by running like that it compiles the program into bytecode and then bytecode will be executed.

Development Cycle – Compile source code to jar using SBT

As part of this topic, we will build the jar file and run as the jar file

For building the jar file we typically use a tool called SBT
SBT stands for a simple build tool
Scala applications can be compiled in many other ways but SBT is the most popular one

Set up SBT
Go to this site to download the SBT
Install and download the sbt as mentioned in the site
Launch sbt by hitting sbt in the terminal
Go to the path(in my case its /Research/code/scalademo/retail) and enter the command sbt package. It will compile the source code
Once it is compiled we can run the program by using the command sbt run.

As part of this topic, we will explore another way of compiling and running the jar file with arguments through SBT

We can make the changes to the code
Recompile the jar file
Run the jar file
How to pass the arguments to the program

import scala.io.Source
object OrderRevenue {
def main(args: Array[String]) = {
  val orderId = args(1).toInt
  val orderItems = Source.fromFile("/home/dgadiraju/data/retail_db/order_items/part-00000").getLines
  val orderRevenue = orderItems.filter(oi => oi.split(",")(1).toInt == orderId).
  map(oi => oi.split(",")(4).toFloat).
  reduce((t, v) => t + v)
  println(orderRevenue)
 }
}
Run the program using this below command

scala target/scala-2.10/retail_2.10-1.0.jar OrderRevenue 2

Setting up IntelliJ with scala
Go to the link Scala Download
Click on Download IntelliJ option
Go to the bottom of the page and then click on the community edition of windows .exe file
Install the IntelliJ file and follow the procedure
Open IntelliJ and click on plugins
Search for scala and install the plugin and click on ok
Give project details as mentioned

Create a scala program
Right click on scala -> new -> scala class
Choose scala object and give the object name and hit enter
Copy the code which we have seen earlier
Pass the arguments as Run ->edit configurations, program arguments

Run the program
Example
import scala.io.Source
object OrderRevenue {
  def main(args: Array[String]) = {
    val orderId = args(0).toInt
    val orderItems = Source.fromFile("/home/dgadiraju/data/retail_db/order_items/part-00000").getLines
    val orderRevenue = orderItems.filter(oi => oi.split(",")(1).toInt == orderId).
      map(oi => oi.split(",")(4).toFloat).
      reduce((t, v) => t + v)
    println(orderRevenue)
  }
}

------------


Transform, Stage and Store - Spark

spark- distributed computing framework(bunch of apis to process data)

it can read data from hdfs, s3 ,azure vaiery of cloud patforms



Create Resilient Distributed Data Sets (RDD)

RDD – Resilient Distributed Dataset
In-memory
Distributed
Resilient
Reading files from HDFS
Reading files from the local file system and create RDD
A quick overview of Transformations and Actions
DAG and lazy evaluation
Previewing the data using Actions

Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.

Resilient, means that it can be recomputed in case Spark looses a part of the data. RDD's are computed in memory, can be persisted in memory.
Formally, an RDD is a read-only, partitioned collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.


Creating RDD -Validating files from a file system

hadoop fs -ls /public/reatil_db/orders
hadoop fs -tail /public/retail_db/orders/part-00000


create RDD using spark-shell

spark-shell --master yarn \
 --conf spark.ui.port = 12654 \
 --num-executors 1 \
 --executor-memory 512M

val orders = sc.textFile("/public/retail_db/orders")
To see first 10 records from the table we use orders.take(10)

Reading files from the local file system and creating RDD

val productsRaw = scala.io.Source.fromFile("/data/retail_db/products/part-00000").getLines.toList
val productsRDD = sc.parallelize(productsRaw)
productsRDD.take(10)



Data Frame (Distributed collection with structure)

APIs provided by sqlContext
Supported file format
orc
json
parquet
avro
Previewing the data
show
Creating Data Frame using sqlContext
To preview the data from orders table give the command hadoop fs -tail /public/retail_db_json/orders/part-r-00000-990f5773-9005-49ba-b670-631286032674
Reading the json file into a variable val ordersDF = sqlContext.read.json(“/public/retail_db_json/orders”)
To preview data use variableName.show ordersDF.show, here it fetches the top 20 rows from the table
To display column names of a table use the command ordersDF.printSchema
To display only two columns from an existing table use the command ordersDF.select(“order_id”, “order_date”).show, here select is an sqlContext api
And there is another way to display the data within a single command by using another sqlContext api load, here is the command sqlContext.load(“/public/retail_db_json/orders”, “json”).show.


Transformations Overview

Transformations Overview
posted on APRIL 27, 2018

Topic Progress:                                        
← Back to Lesson
As part of this topic, we will explore Standard Transformations

String Manipulation (Scala)
Row-level transformations
Filtering (horizontal and vertical)
Joins
Aggregation
Sorting
Ranking
Set Operations


//String Manipulation
val str = orders.first
val a = str.split(",")
val orderId = a(0).toInt
a(1).contains("2013")

val orderDate = a(1)
orderDate.substring(0, 10)
orderDate.substring(5, 7)
orderDate.substring(11)
orderDate.replace('-', '/')
orderDate.replace("07", "July")
orderDate.indexOf("2")
orderDate.indexOf("2", 2)
orderDate.length

Row level transformations using map

Data cleansing – removing special characters
Standardization – eg: phone number, we might want to get phone numbers from different sources and it might be represented the different manner in different systems. When we get onto downstream systems we have to represent phone number in one standard format.
Discarding or filtering out unnecessary data
Unpivoting the data, one row with many columns might have to return a collection of rows

Let us map these scenarios with APIs categorized under transformations

Data cleansing and standardization – map. It takes one record as input and returns exactly one record as output
Discarding or filtering – filter. It takes one record as input, and if the expression returns false record will be discarded
Also for vertical filtering, we use the map function
Unpivoting – flatMap. For each input record, there will be 1 to n output records
Number of invocations = Number of elements in RDD

/Users/pkum60/IdeaProjects/getstartsprk
sbt package
/Users/pkum60/IdeaProjects/getstartsprk/target/scala-2.11/getstartsprk_2.11-0.1.jar


run sparkjob using spark-submit

spark-submit --class com.test.retailsprk.RetailMainSprk getstartsprk_2.11-0.1.jar local srcpath tgtpath

 spark-submit \
--master yarn \
--deploy-mode cluster \  
--class com.test.retailsprk.RetailMainSprk \
getstartsprk_2.11-0.1.jar s3://digital-ingress-preprod-east/cde/dev/member-access/tst1/retail_db/order_items/ s3://digital-ingress-preprod-east/cde/dev/member-access/tst1/retail_db/order_items_out/

spark-submit \
--master yarn \
--deploy-mode cluster \  
--class com.test.retailsprk.RetailMainSprk \
getstartsprk_2.11-0.1.jar s3://digital-ingress-preprod-east/cde/dev/member-access/tst1/retail_db/order_items/ s3://digital-ingress-preprod-east/cde/dev/member-access/tst1/retail_db/order_items_out/

